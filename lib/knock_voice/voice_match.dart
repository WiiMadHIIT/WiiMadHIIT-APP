import 'dart:math';
import 'package:tflite_flutter/tflite_flutter.dart';
import 'package:flutter_sound/flutter_sound.dart';
import 'package:flutter_sound_processing/flutter_sound_processing.dart';

/// VoiceMatch - è¯­éŸ³åŒ¹é…å™¨
/// ä½¿ç”¨ x-vector TFLite æ¨¡å‹è¿›è¡Œè¯­éŸ³ç‰¹å¾æå–å’Œç›¸ä¼¼åº¦è®¡ç®—
/// åŸºäº SpeechBrain x-vector æ¨¡å‹è½¬æ¢è€Œæ¥
class VoiceMatch {
  Interpreter? _interpreter;
  bool _isLoaded = false;
  
  /// æ¨¡å‹æ˜¯å¦å·²åŠ è½½
  bool get isLoaded => _isLoaded;
  
  /// åŠ è½½ x-vector TFLite æ¨¡å‹
  Future<bool> loadModel() async {
    try {
      print('ğŸµ Loading x-vector TFLite model...');
      _interpreter = await Interpreter.fromAsset('assets/model/x_vector.tflite');
      
      // è·å–è¾“å…¥è¯¦æƒ…
      var inputDetails = _interpreter!.getInputDetails();
      print('ğŸµ Input details: ${inputDetails.length} inputs');
      
      // è®¾ç½®åŠ¨æ€è¾“å…¥å½¢çŠ¶ [1, -1, 24] (batch_size, time_steps, mfcc_features)
      _interpreter!.resizeInputTensor(inputDetails[0]['index'], [1, -1, 24]);
      _interpreter!.allocateTensors();
      
      _isLoaded = true;
      print('ğŸµ x-vector TFLite model loaded successfully');
      return true;
    } catch (e) {
      print('âŒ Failed to load x-vector TFLite model: $e');
      _isLoaded = false;
      return false;
    }
  }
  
  /// ä»éŸ³é¢‘æ–‡ä»¶æå–è¯­éŸ³åµŒå…¥å‘é‡
  Future<List<double>> extractEmbeddingFromFile(String audioPath) async {
    if (!_isLoaded) {
      throw Exception('Model not loaded. Call loadModel() first.');
    }
    
    try {
      print('ğŸµ Extracting embedding from file: $audioPath');
      
      var audio = FlutterSound();
      await audio.openAudioSession();
      
      // å¼€å§‹æ’­æ”¾å™¨æ¥è·å–éŸ³é¢‘ä¿¡å·
      var signal = await audio.startPlayer(audioPath);
      
      // æå– MFCC ç‰¹å¾
      var mfcc = await FlutterSoundProcessing().extractMFCC(
        signal,
        sampleRate: 16000,
        nMfcc: 24,
        hopLength: 160,
        winLength: 400
      );
      
      print('ğŸµ MFCC shape: ${mfcc.shape}');
      
      // é‡å¡‘ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ [1, time_steps, 24]
      var inputMfcc = mfcc.reshape([1, mfcc.length, 24]);
      
      // å‡†å¤‡è¾“å‡ºå¼ é‡ [1, 512]
      var output = List.filled(512, 0.0).reshape([1, 512]);
      
      // è®¾ç½®è¾“å…¥å¼ é‡
      _interpreter!.setTensor(
        _interpreter!.getInputIndex('serving_default_mfcc:0'), 
        inputMfcc
      );
      
      // è¿è¡Œæ¨ç†
      _interpreter!.invoke();
      
      // è·å–è¾“å‡º
      output = _interpreter!.getOutputTensor(
        _interpreter!.getOutputIndex('StatefulPartitionedCall:0')
      );
      
      await audio.closeAudioSession();
      
      print('ğŸµ Embedding extracted successfully: ${output[0].length} dimensions');
      return output[0];
    } catch (e) {
      print('âŒ Failed to extract embedding from file: $e');
      rethrow;
    }
  }
  
  /// ä»éŸ³é¢‘æ•°æ®æå–è¯­éŸ³åµŒå…¥å‘é‡
  Future<List<double>> extractEmbeddingFromAudioData(List<double> audioData, {int sampleRate = 16000}) async {
    if (!_isLoaded) {
      throw Exception('Model not loaded. Call loadModel() first.');
    }
    
    try {
      print('ğŸµ Extracting embedding from audio data: ${audioData.length} samples');
      
      // ä½¿ç”¨ FlutterSoundProcessing æå– MFCC
      var mfcc = await FlutterSoundProcessing().extractMFCCFromSamples(
        audioData,
        sampleRate: sampleRate,
        nMfcc: 24,
        hopLength: 160,
        winLength: 400
      );
      
      print('ğŸµ MFCC shape: ${mfcc.shape}');
      
      // é‡å¡‘ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ [1, time_steps, 24]
      var inputMfcc = mfcc.reshape([1, mfcc.length, 24]);
      
      // å‡†å¤‡è¾“å‡ºå¼ é‡ [1, 512]
      var output = List.filled(512, 0.0).reshape([1, 512]);
      
      // è®¾ç½®è¾“å…¥å¼ é‡
      _interpreter!.setTensor(
        _interpreter!.getInputIndex('serving_default_mfcc:0'), 
        inputMfcc
      );
      
      // è¿è¡Œæ¨ç†
      _interpreter!.invoke();
      
      // è·å–è¾“å‡º
      output = _interpreter!.getOutputTensor(
        _interpreter!.getOutputIndex('StatefulPartitionedCall:0')
      );
      
      print('ğŸµ Embedding extracted successfully: ${output[0].length} dimensions');
      return output[0];
    } catch (e) {
      print('âŒ Failed to extract embedding from audio data: $e');
      rethrow;
    }
  }
  
  /// è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
  double cosineSimilarity(List<double> emb1, List<double> emb2) {
    if (emb1.length != emb2.length) {
      throw Exception('Embedding dimensions must match: ${emb1.length} vs ${emb2.length}');
    }
    
    double dotProduct = 0.0;
    double norm1 = 0.0;
    double norm2 = 0.0;
    
    for (int i = 0; i < emb1.length; i++) {
      dotProduct += emb1[i] * emb2[i];
      norm1 += emb1[i] * emb1[i];
      norm2 += emb2[i] * emb2[i];
    }
    
    double similarity = dotProduct / (sqrt(norm1) * sqrt(norm2));
    
    // ç¡®ä¿ç»“æœåœ¨ [-1, 1] èŒƒå›´å†…
    return similarity.clamp(-1.0, 1.0);
  }
  
  /// è®¡ç®—å½’ä¸€åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦ (0-1 èŒƒå›´)
  double normalizedCosineSimilarity(List<double> emb1, List<double> emb2) {
    double similarity = cosineSimilarity(emb1, emb2);
    // å°† [-1, 1] è½¬æ¢ä¸º [0, 1]
    return (similarity + 1.0) / 2.0;
  }
  
  /// é‡Šæ”¾èµ„æº
  void dispose() {
    _interpreter?.close();
    _interpreter = null;
    _isLoaded = false;
    print('ğŸµ VoiceMatch disposed');
  }
} 