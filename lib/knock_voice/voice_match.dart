import 'dart:math';
import 'package:tflite_flutter/tflite_flutter.dart';
import 'package:flutter_sound/flutter_sound.dart';

/// VoiceMatch - è¯­éŸ³åŒ¹é…å™¨
/// ä½¿ç”¨ x-vector TFLite æ¨¡å‹è¿›è¡Œè¯­éŸ³ç‰¹å¾æå–å’Œç›¸ä¼¼åº¦è®¡ç®—
/// åŸºäº SpeechBrain x-vector æ¨¡å‹è½¬æ¢è€Œæ¥
class VoiceMatch {
  Interpreter? _interpreter;
  bool _isLoaded = false;
  
  /// æ¨¡å‹æ˜¯å¦å·²åŠ è½½
  bool get isLoaded => _isLoaded;
  
  /// åŠ è½½ x-vector TFLite æ¨¡å‹
  Future<bool> loadModel() async {
    try {
      print('ğŸµ Loading x-vector TFLite model...');
      _interpreter = await Interpreter.fromAsset('assets/model/x_vector.tflite');
      
      // è·å–è¾“å…¥è¯¦æƒ…
      var inputDetails = _interpreter!.getInputDetails();
      print('ğŸµ Input details: ${inputDetails.length} inputs');
      
      // è®¾ç½®åŠ¨æ€è¾“å…¥å½¢çŠ¶ [1, -1, 24] (batch_size, time_steps, mfcc_features)
      _interpreter!.resizeInputTensor(inputDetails[0]['index'], [1, -1, 24]);
      _interpreter!.allocateTensors();
      
      _isLoaded = true;
      print('ğŸµ x-vector TFLite model loaded successfully');
      return true;
    } catch (e) {
      print('âŒ Failed to load x-vector TFLite model: $e');
      _isLoaded = false;
      return false;
    }
  }
  
  /// ä»éŸ³é¢‘æ–‡ä»¶æå–è¯­éŸ³åµŒå…¥å‘é‡
  Future<List<double>> extractEmbeddingFromFile(String audioPath) async {
    if (!_isLoaded) {
      throw Exception('Model not loaded. Call loadModel() first.');
    }
    
    try {
      print('ğŸµ Extracting embedding from file: $audioPath');
      
      // ç®€åŒ–çš„æ–‡ä»¶å¤„ç†ï¼ˆæš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®ï¼‰
      // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä½ éœ€è¦å®ç°éŸ³é¢‘æ–‡ä»¶çš„è¯»å–å’Œç‰¹å¾æå–
      List<double> mockFeatures = List.filled(24, 0.1);
      
      // é‡å¡‘ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ [1, 1, 24]
      var inputFeatures = _reshapeFeatures(mockFeatures);
      
      // å‡†å¤‡è¾“å‡ºå¼ é‡ [1, 512]
      var output = List.filled(512, 0.0).reshape([1, 512]);
      
      // è®¾ç½®è¾“å…¥å¼ é‡
      _interpreter!.setTensor(
        _interpreter!.getInputIndex('serving_default_mfcc:0'), 
        inputFeatures
      );
      
      // è¿è¡Œæ¨ç†
      _interpreter!.invoke();
      
      // è·å–è¾“å‡º
      output = _interpreter!.getOutputTensor(
        _interpreter!.getOutputIndex('StatefulPartitionedCall:0')
      );
      
      print('ğŸµ Embedding extracted successfully: ${output[0].length} dimensions');
      return output[0];
    } catch (e) {
      print('âŒ Failed to extract embedding from file: $e');
      rethrow;
    }
  }
  
  /// ä»éŸ³é¢‘æ•°æ®æå–è¯­éŸ³åµŒå…¥å‘é‡
  Future<List<double>> extractEmbeddingFromAudioData(List<double> audioData, {int sampleRate = 16000}) async {
    if (!_isLoaded) {
      throw Exception('Model not loaded. Call loadModel() first.');
    }
    
    try {
      print('ğŸµ Extracting embedding from audio data: ${audioData.length} samples');
      
      // ç®€åŒ–çš„éŸ³é¢‘ç‰¹å¾æå–ï¼ˆæ›¿ä»£ MFCCï¼‰
      var features = _extractSimpleAudioFeatures(audioData, sampleRate);
      
      print('ğŸµ Features shape: ${features.length}');
      
      // é‡å¡‘ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ [1, time_steps, 24]
      var inputFeatures = _reshapeFeatures(features);
      
      // å‡†å¤‡è¾“å‡ºå¼ é‡ [1, 512]
      var output = List.filled(512, 0.0).reshape([1, 512]);
      
      // è®¾ç½®è¾“å…¥å¼ é‡
      _interpreter!.setTensor(
        _interpreter!.getInputIndex('serving_default_mfcc:0'), 
        inputFeatures
      );
      
      // è¿è¡Œæ¨ç†
      _interpreter!.invoke();
      
      // è·å–è¾“å‡º
      output = _interpreter!.getOutputTensor(
        _interpreter!.getOutputIndex('StatefulPartitionedCall:0')
      );
      
      print('ğŸµ Embedding extracted successfully: ${output[0].length} dimensions');
      return output[0];
    } catch (e) {
      print('âŒ Failed to extract embedding from audio data: $e');
      rethrow;
    }
  }
  
  /// ç®€åŒ–çš„éŸ³é¢‘ç‰¹å¾æå–
  List<double> _extractSimpleAudioFeatures(List<double> audioData, int sampleRate) {
    if (audioData.isEmpty) return List.filled(24, 0.0);
    
    // è®¡ç®—åŸºæœ¬éŸ³é¢‘ç‰¹å¾
    List<double> features = [];
    
    // 1. RMS èƒ½é‡
    double rms = _calculateRMS(audioData);
    features.add(rms);
    
    // 2. é¢‘è°±è´¨å¿ƒï¼ˆç®€åŒ–ç‰ˆï¼‰
    double spectralCentroid = _calculateSpectralCentroid(audioData);
    features.add(spectralCentroid);
    
    // 3. è¿‡é›¶ç‡
    double zeroCrossingRate = _calculateZeroCrossingRate(audioData);
    features.add(zeroCrossingRate);
    
    // 4. é¢‘è°±æ»šé™ï¼ˆç®€åŒ–ç‰ˆï¼‰
    double spectralRolloff = _calculateSpectralRolloff(audioData);
    features.add(spectralRolloff);
    
    // 5. å¡«å……åˆ° 24 ç»´ï¼ˆæ¨¡æ‹Ÿ MFCCï¼‰
    while (features.length < 24) {
      features.add(0.0);
    }
    
    return features.take(24).toList();
  }
  
  /// é‡å¡‘ç‰¹å¾ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
  List<List<List<double>>> _reshapeFeatures(List<double> features) {
    // åˆ›å»º [1, 1, 24] çš„å½¢çŠ¶
    return [[features]];
  }
  
  /// è®¡ç®— RMS
  double _calculateRMS(List<double> data) {
    if (data.isEmpty) return 0.0;
    double sum = 0.0;
    for (var sample in data) {
      sum += sample * sample;
    }
    return sqrt(sum / data.length);
  }
  
  /// è®¡ç®—é¢‘è°±è´¨å¿ƒï¼ˆç®€åŒ–ç‰ˆï¼‰
  double _calculateSpectralCentroid(List<double> audioData) {
    if (audioData.isEmpty) return 0.0;
    
    double weightedSum = 0.0;
    double sum = 0.0;
    
    for (int i = 0; i < audioData.length; i++) {
      double magnitude = audioData[i].abs();
      weightedSum += magnitude * i;
      sum += magnitude;
    }
    
    return sum > 0 ? weightedSum / sum : 0.0;
  }
  
  /// è®¡ç®—è¿‡é›¶ç‡
  double _calculateZeroCrossingRate(List<double> audioData) {
    if (audioData.length < 2) return 0.0;
    
    int crossings = 0;
    for (int i = 1; i < audioData.length; i++) {
      if ((audioData[i] >= 0) != (audioData[i - 1] >= 0)) {
        crossings++;
      }
    }
    
    return crossings / (audioData.length - 1);
  }
  
  /// è®¡ç®—é¢‘è°±æ»šé™ï¼ˆç®€åŒ–ç‰ˆï¼‰
  double _calculateSpectralRolloff(List<double> audioData) {
    if (audioData.isEmpty) return 0.0;
    
    // æ’åºå¹…åº¦
    List<double> magnitudes = audioData.map((e) => e.abs()).toList();
    magnitudes.sort();
    
    // æ‰¾åˆ° 85% åˆ†ä½æ•°
    int index = (magnitudes.length * 0.85).round();
    if (index >= magnitudes.length) index = magnitudes.length - 1;
    
    return magnitudes[index];
  }
  
  /// è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
  double cosineSimilarity(List<double> emb1, List<double> emb2) {
    if (emb1.length != emb2.length) {
      throw Exception('Embedding dimensions must match: ${emb1.length} vs ${emb2.length}');
    }
    
    double dotProduct = 0.0;
    double norm1 = 0.0;
    double norm2 = 0.0;
    
    for (int i = 0; i < emb1.length; i++) {
      dotProduct += emb1[i] * emb2[i];
      norm1 += emb1[i] * emb1[i];
      norm2 += emb2[i] * emb2[i];
    }
    
    double similarity = dotProduct / (sqrt(norm1) * sqrt(norm2));
    
    // ç¡®ä¿ç»“æœåœ¨ [-1, 1] èŒƒå›´å†…
    return similarity.clamp(-1.0, 1.0);
  }
  
  /// è®¡ç®—å½’ä¸€åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦ (0-1 èŒƒå›´)
  double normalizedCosineSimilarity(List<double> emb1, List<double> emb2) {
    double similarity = cosineSimilarity(emb1, emb2);
    // å°† [-1, 1] è½¬æ¢ä¸º [0, 1]
    return (similarity + 1.0) / 2.0;
  }
  
  /// é‡Šæ”¾èµ„æº
  void dispose() {
    _interpreter?.close();
    _interpreter = null;
    _isLoaded = false;
    print('ğŸµ VoiceMatch disposed');
  }
} 